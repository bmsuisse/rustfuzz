{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Blazing-fast fuzzy string matching \u2014 implemented entirely in Rust. Built entirely by AI. Designed to beat RapidFuzz. </p>"},{"location":"#the-story","title":"The Story","text":"<p>rustfuzz started as an experiment: can an AI agent, starting from scratch, build a fuzzy-matching library that outperforms RapidFuzz \u2014 one of the best-optimised C++ string-matching libraries in the Python ecosystem?</p> <p>No human wrote the Rust. No human tuned the algorithm parameters. The AI drove every iteration, read every benchmark result, and decided what to rewrite next.</p> <p>The answer the AI kept coming back to: Rust + PyO3 + tight Python-boundary design.</p>"},{"location":"#the-development-loop","title":"The Development Loop","text":"<p>Every feature and optimisation went through the same cycle:</p> <pre><code>flowchart LR\n    R[\"\ud83d\udd0d Research&lt;br&gt;Profiler output&lt;br&gt;&amp; algorithm gaps\"]\n    B[\"\ud83e\udd80 Build&lt;br&gt;Rust core&lt;br&gt;via PyO3\"]\n    T[\"\u2705 Test&lt;br&gt;All tests must pass&lt;br&gt;before proceeding\"]\n    BM[\"\ud83d\udcca Benchmark&lt;br&gt;vs RapidFuzz&lt;br&gt;&amp; record results\"]\n    RP[\"\ud83d\udd01 Repeat&lt;br&gt;Find the next&lt;br&gt;bottleneck\"]\n\n    R --&gt; B --&gt; T --&gt; BM --&gt; RP --&gt; R\n\n    style R fill:#6366f1,color:#fff,stroke:none\n    style B fill:#a855f7,color:#fff,stroke:none\n    style T fill:#ef4444,color:#fff,stroke:none\n    style BM fill:#22c55e,color:#fff,stroke:none\n    style RP fill:#f59e0b,color:#fff,stroke:none</code></pre> <p>Each iteration asked:</p> <ul> <li>Research \u2014 where is the remaining Python overhead? What does the profiler show?</li> <li>Build \u2014 move that hot path into Rust. Eliminate copies, reduce allocations, avoid iterator protocol overhead.</li> <li>Test \u2014 the full test suite must pass before proceeding. No broken correctness, no skipped edge cases.</li> <li>Benchmark \u2014 run head-to-head comparisons vs RapidFuzz. Numbers don't lie.</li> <li>Repeat \u2014 the next bottleneck is always waiting.</li> </ul>"},{"location":"#why-this-matters","title":"Why This Matters","text":"<p>RapidFuzz is exceptional \u2014 its C++ core, SIMD intrinsics, and decades of optimisation make it a formidable target. The goal of this project was never to dismiss it, but to prove that:</p> <ol> <li>AI can drive non-trivial systems programming \u2014 not just generate boilerplate.</li> <li>Rust + PyO3 can match C++ at the Python boundary \u2014 with the added safety guarantees Rust provides.</li> <li>Iterative AI-driven optimisation works \u2014 each benchmark loop produced measurable gains.</li> </ol>"},{"location":"#features","title":"Features","text":"\u26a1 Blazing Fast Core algorithms in Rust \u2014 no Python overhead, no GIL bottlenecks \ud83e\udde0 Smart Matching ratio, partial_ratio, token sort/set, Levenshtein, Jaro-Winkler, and more \ud83d\udd12 Memory Safe Rust's borrow checker \u2014 no segfaults, no buffer overflows \ud83d\udc0d Pythonic API Typed Python interface \u2014 <code>import rustfuzz.fuzz as fuzz</code> and go \ud83d\udce6 No Build Step Pre-compiled wheels for Python 3.10\u20133.13 on Linux, macOS, and Windows \ud83c\udfd4\ufe0f Big Data Ready Excels in 1 Billion Row Challenge benchmarks, crushing high-throughput tasks \ud83e\udde9 Ecosystem Integrations BM25, Hybrid Search, and LangChain Retrievers for Vector DBs"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install rustfuzz\n# or with uv:\nuv pip install rustfuzz\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import rustfuzz.fuzz as fuzz\nfrom rustfuzz.distance import Levenshtein, JaroWinkler\nfrom rustfuzz import process\n\n# Similarity ratios\nfuzz.ratio(\"hello world\", \"hello wrold\")            # ~96.0\nfuzz.partial_ratio(\"hello\", \"say hello world\")      # 100.0\nfuzz.token_sort_ratio(\"fuzzy wuzzy\", \"wuzzy fuzzy\") # 100.0\n\n# Edit distance\nLevenshtein.distance(\"kitten\", \"sitting\")           # 3\nJaroWinkler.similarity(\"martha\", \"marhta\")          # ~0.96\n\n# Batch matching\nprocess.extractOne(\"new york\", [\"New York\", \"Newark\", \"Los Angeles\"])\n# ('New York', 100.0, 0)\n</code></pre>"},{"location":"#cookbook-recipes","title":"Cookbook Recipes \ud83e\uddd1\u200d\ud83c\udf73","text":"Recipe Description Introduction Get started \u2014 basic matching and terminology Advanced Matching Partial ratios, token sorts, score cutoffs Benchmarks Head-to-head speed comparisons vs RapidFuzz <p>Start exploring from the navigation menu on the left!</p>"},{"location":"architecture/","title":"Architecture &amp; Design","text":"<p><code>rustfuzz</code> is built with a singular goal: provide the fastest fuzzy string matching library for Python by moving all heavy computation into Rust, while maintaining a perfectly Pythonic API.</p> <p>This document outlines the core architectural decisions that make <code>rustfuzz</code> incredibly fast and memory-safe.</p>"},{"location":"architecture/#1-rust-ffi-pyo3-bindings","title":"1. Rust FFI &amp; PyO3 Bindings","text":"<p>The bridge between Python and Rust is built using <code>PyO3</code>. We prioritize zero-copy or minimal-copy extraction of Python strings.</p>"},{"location":"architecture/#string-extraction","title":"String Extraction","text":"<p>When a Python string is passed to a Rust function, we avoid allocating new Rust <code>String</code> objects if possible.  - For ASCII strings (<code>PyUnicode_1BYTE_KIND</code>), we extract direct pointers to the underlying byte array using <code>PyUnicode_AsUTF8AndSize</code> and process them as <code>&amp;[u8]</code>. - This ensures that operations like <code>fuzz.ratio</code> have zero allocation overhead for the strings themselves.</p>"},{"location":"architecture/#apache-arrow-pycapsule-management","title":"Apache Arrow &amp; PyCapsule Management","text":"<p>For high-throughput analytical workloads (e.g., PySpark DataFrames), zero-copy data transfer is facilitated using the Apache Arrow C-Data interface. We strictly manage <code>PyCapsule</code> lifetimes during FFI boundary crossings to ensure that memory references remain valid and are not prematurely garbage collected by Python, completely avoiding segmentation faults during batch processing.</p>"},{"location":"architecture/#2-core-algorithms-optimizations","title":"2. Core Algorithms &amp; Optimizations","text":"<p>The <code>src/algorithms.rs</code> and <code>src/fuzz.rs</code> files contain highly optimized versions of standard fuzzy matching algorithms.</p> <ul> <li>Levenshtein Distance: We use variations of Myers' bit-parallel algorithm (<code>PatternMask64</code>) for strings up to 64 characters long, significantly outperforming naive dynamic programming approaches. </li> <li>Pattern Masks: The <code>PatternMask64</code> builds a 64-bit integer mask for each character in the query, allowing multiple characters to be processed in a single CPU instruction during the matching matrix calculation.</li> <li>Fast Paths: Natively supported scorers (e.g., <code>ratio</code>, <code>wratio</code>, <code>partial_ratio</code>) completely bypass Python callback overhead when used in batch processing methods like <code>process.extract</code>.</li> </ul>"},{"location":"architecture/#3-batch-processing-processrs","title":"3. Batch Processing (<code>process.rs</code>)","text":"<p>Batch processing is the area where <code>rustfuzz</code> delivers the most significant performance gains over pure Python or Cython implementations.</p>"},{"location":"architecture/#the-fast-path","title":"The Fast Path","text":"<p>When <code>process.extract</code> evaluates a query against a list of choices, it does the following: 1. Validates if the selected scorer is implemented natively (<code>ScorerType</code>). 2. If the scorer is native and no custom Python <code>processor</code> function is provided, it enters the Native Fast Path (<code>ratio_fast</code>). 3. For large lists (defined by <code>PARALLEL_THRESHOLD = 64</code>), it switches to a Rayon-powered parallel execution model.</p>"},{"location":"architecture/#concurrency-and-the-gil","title":"Concurrency and the GIL","text":"<ul> <li>Phase 1 (GIL Held): Safely extract raw byte slice pointers (<code>*mut PyObject</code>) from the Python list.</li> <li>Phase 2 (GIL Released): Releases the Python Global Interpreter Lock (<code>py.allow_threads()</code>). Spawns a Rayon parallel iterator (<code>par_iter</code>) that evaluates the distance metric across all available CPU cores. No Python objects are interacted with during this phase.</li> <li>Phase 3 (GIL Held): Re-acquire the GIL and pack the resulting winner pairs <code>(PyObject, score, index)</code> into a final Python list.</li> </ul> <p>This architecture scales linearly with CPU cores for functions like <code>cdist</code> (distance matrix generation) and <code>extract_iter</code>.</p>"},{"location":"architecture/#4-advanced-data-structures","title":"4. Advanced Data Structures","text":""},{"location":"architecture/#bk-trees-for-deduplication","title":"BK-Trees for Deduplication","text":"<p>Deduplication is handled natively in Rust using a Burkhard-Keller Tree (<code>src/distance/bktree.rs</code>). The tree organizes strings based on a discrete metric distance (Levenshtein). This turns an $O(N^2)$ deduplication problem into a vastly faster tree traversal where entire branches are pruned based on the triangle inequality.</p>"},{"location":"architecture/#hybrid-search-bm25","title":"Hybrid Search &amp; BM25","text":"<p>For integration with LLMs and Vector databases, <code>src/search.rs</code> provides a native <code>BM25Index</code>. - It implements the Okapi BM25 scoring algorithm, parallelized using Rayon over document chunks. - RRF (Reciprocal Rank Fusion): Provides hybrid capabilities (<code>get_top_n_rrf</code>) that effectively merge traditional lexical token matching (BM25) with character-level fuzzy matching, providing robust retrieval even when queries include typos.</p>"},{"location":"developer_guide/","title":"Developer &amp; Contributor Guide","text":"<p>Welcome to the <code>rustfuzz</code> project! This guide explains how to build, test, and contribute to the library. The project follows strict guidelines to ensure maximum performance, safety, and maintainability.</p>"},{"location":"developer_guide/#1-toolchain-setup","title":"1. Toolchain &amp; Setup","text":"<p>We use modern, fast tools built in Rust across the entire stack: - Rust / Cargo: For the underlying implementations. - Maturin: For building Python wheels and managing the PyO3 bindings. - uv: For incredibly fast Python environment and dependency management. - Ruff: For Python linting and formatting. - Pyright: For strict static type checking in Python.</p>"},{"location":"developer_guide/#initial-setup","title":"Initial Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/bmsuisse/rustfuzz.git\ncd rustfuzz\n\n# Create a virtual environment and sync dependencies using uv\nuv sync --all-groups\n</code></pre> <p>To build the development version of the Rust extension:</p> <pre><code>uv run maturin develop\n# Use --release for benchmarking to disable debug assertions and enable optimizations\nuv run maturin develop --release\n</code></pre>"},{"location":"developer_guide/#2-the-core-optimization-loop","title":"2. The Core Optimization Loop","text":"<p>Our development philosophy is summarized in the following loop, designed to aggressively optimize performance vs. benchmark targets (like RapidFuzz):</p> <p>Research \u2192 Build \u2192 Test \u2192 Benchmark \u2192 Repeat</p> <ol> <li>Research: Run profilers to identify bottlenecks or missing algorithm fast-paths.</li> <li>Build: Implement the logic in Rust and expose it via PyO3.</li> <li>Test: Validate against existing tests to ensure 100% equivalence and correctness (Memory safety).</li> <li>Benchmark: Run the <code>pytest-benchmark</code> suite to measure the impact.</li> <li>Repeat: Repeat the cycle.</li> </ol> <p>Note: You can trigger the automated benchmark workflow via the <code>/research-build-test-benchmark</code> command if using the dedicated AI agent flow.</p>"},{"location":"developer_guide/#3-coding-standards-rules","title":"3. Coding Standards &amp; Rules","text":"<p>To maintain high code quality, please adhere strictly to the following rules:</p> <ul> <li>1000 Lines Max: No file may exceed 1000 lines of code. If a module grows too large, refactor it into smaller, manageable pieces to ensure cognitive load remains low.</li> <li>Branching: Create a dedicated branch for every new feature or bugfix. Do not push directly to <code>main</code>.</li> <li>Typing First: Python features must be fully typed. Use <code>pyright</code> to catch typing errors before committing.    <pre><code>uv run pyright\n</code></pre></li> <li>Linting: We enforce <code>ruff</code> for linting.    <pre><code>uv run ruff check .\n</code></pre></li> </ul>"},{"location":"developer_guide/#4-testing-qa","title":"4. Testing &amp; QA","text":"<ul> <li>Ensure tests pass before committing: <pre><code>uv run pytest\n</code></pre></li> <li>End-to-End (E2E) Verification: Whenever implementing a major new feature (like a new core algorithm or architectural integration), write comprehensive unit tests or E2E tests validating the full pipeline from Python through FFI into Rust and back out.</li> </ul>"},{"location":"developer_guide/#5-webbrowser-verification-if-applicable","title":"5. Web/Browser Verification (If Applicable)","text":"<p><code>rustfuzz</code> is strictly a backend/library tool. However, if visual output or browser-based dashboards are added in the future (e.g., for reporting), never attempt manual browser testing. Always use Playwright for automated, reproducible E2E browser tests. </p>"},{"location":"developer_guide/#6-documenting-functions","title":"6. Documenting Functions","text":"<ul> <li>Python wrappers must have standard docstrings.</li> <li>Ensure any new Python stubs (<code>.pyi</code>) are kept perfectly in sync with the <code>src/*.rs</code> Rust implementation signatures.</li> </ul>"},{"location":"cookbook/01_introduction/","title":"Introduction to rustfuzz","text":"In\u00a0[\u00a0]: Copied! <pre>import rustfuzz.fuzz as fuzz\nimport rustfuzz.utils as utils\nfrom rustfuzz import process\n\nprint(\"rustfuzz ready \u2705\")\n</pre> import rustfuzz.fuzz as fuzz import rustfuzz.utils as utils from rustfuzz import process  print(\"rustfuzz ready \u2705\") In\u00a0[\u00a0]: Copied! <pre>pairs = [\n    (\"hello world\", \"hello world\"),   # identical\n    (\"hello world\", \"hello wrold\"),   # one transposition\n    (\"hello world\", \"hello\"),         # truncated\n    (\"New York\",    \"New York City\"), # prefix\n    (\"kitten\",      \"sitting\"),       # classic Levenshtein example\n    (\"apple\",       \"mango\"),         # unrelated\n]\n\nfor a, b in pairs:\n    print(f\"{a!r:25} vs {b!r:25}  \u2192 ratio = {fuzz.ratio(a, b):.1f}\")\n</pre> pairs = [     (\"hello world\", \"hello world\"),   # identical     (\"hello world\", \"hello wrold\"),   # one transposition     (\"hello world\", \"hello\"),         # truncated     (\"New York\",    \"New York City\"), # prefix     (\"kitten\",      \"sitting\"),       # classic Levenshtein example     (\"apple\",       \"mango\"),         # unrelated ]  for a, b in pairs:     print(f\"{a!r:25} vs {b!r:25}  \u2192 ratio = {fuzz.ratio(a, b):.1f}\") In\u00a0[\u00a0]: Copied! <pre>needle = \"New York\"\nhaystacks = [\n    \"I live in New York City\",\n    \"New York is great\",\n    \"nyc\",\n    \"Los Angeles\",\n]\n\nfor h in haystacks:\n    r  = fuzz.ratio(needle, h)\n    pr = fuzz.partial_ratio(needle, h)\n    print(f\"{h!r:30}  ratio={r:5.1f}  partial_ratio={pr:5.1f}\")\n</pre> needle = \"New York\" haystacks = [     \"I live in New York City\",     \"New York is great\",     \"nyc\",     \"Los Angeles\", ]  for h in haystacks:     r  = fuzz.ratio(needle, h)     pr = fuzz.partial_ratio(needle, h)     print(f\"{h!r:30}  ratio={r:5.1f}  partial_ratio={pr:5.1f}\") In\u00a0[\u00a0]: Copied! <pre>a = \"fuzzy wuzzy was a bear\"\nb = \"wuzzy fuzzy was a bear\"\n\nprint(f\"ratio              = {fuzz.ratio(a, b):.1f}\")\nprint(f\"token_sort_ratio   = {fuzz.token_sort_ratio(a, b):.1f}\")\nprint(f\"token_set_ratio    = {fuzz.token_set_ratio(a, b):.1f}\")\nprint(f\"token_ratio        = {fuzz.token_ratio(a, b):.1f}\")\n\nprint()\n\n# Subset: token_set_ratio handles extra tokens gracefully\nc = \"fuzzy fuzzy was a bear\"\nprint(f\"token_set_ratio (with duplicate token): {fuzz.token_set_ratio(a, c):.1f}\")\n</pre> a = \"fuzzy wuzzy was a bear\" b = \"wuzzy fuzzy was a bear\"  print(f\"ratio              = {fuzz.ratio(a, b):.1f}\") print(f\"token_sort_ratio   = {fuzz.token_sort_ratio(a, b):.1f}\") print(f\"token_set_ratio    = {fuzz.token_set_ratio(a, b):.1f}\") print(f\"token_ratio        = {fuzz.token_ratio(a, b):.1f}\")  print()  # Subset: token_set_ratio handles extra tokens gracefully c = \"fuzzy fuzzy was a bear\" print(f\"token_set_ratio (with duplicate token): {fuzz.token_set_ratio(a, c):.1f}\") In\u00a0[\u00a0]: Copied! <pre>raw_strings = [\n    \"  Hello, World!  \",\n    \"RustFuzz \u2014 BLAZING FAST!\",\n    \"New York, NY 10001\",\n    None,\n]\n\nfor s in raw_strings:\n    print(f\"{str(s)!r:35} \u2192 {utils.default_process(s)!r}\")\n\nprint()\n# Using processor in ratio call\nscore_raw = fuzz.ratio(\"New York, NY\", \"new york ny\")\nscore_proc = fuzz.ratio(\n    utils.default_process(\"New York, NY\"),\n    utils.default_process(\"new york ny\")\n)\nprint(f\"Without processing: {score_raw:.1f}\")\nprint(f\"With processing:    {score_proc:.1f}\")\n</pre> raw_strings = [     \"  Hello, World!  \",     \"RustFuzz \u2014 BLAZING FAST!\",     \"New York, NY 10001\",     None, ]  for s in raw_strings:     print(f\"{str(s)!r:35} \u2192 {utils.default_process(s)!r}\")  print() # Using processor in ratio call score_raw = fuzz.ratio(\"New York, NY\", \"new york ny\") score_proc = fuzz.ratio(     utils.default_process(\"New York, NY\"),     utils.default_process(\"new york ny\") ) print(f\"Without processing: {score_raw:.1f}\") print(f\"With processing:    {score_proc:.1f}\") In\u00a0[\u00a0]: Copied! <pre>cities = [\n    \"New York\", \"New Orleans\", \"Newark\",\n    \"Los Angeles\", \"San Francisco\", \"Nashville\",\n    \"Boston\", \"Denver\", \"Miami\",\n]\n\nquery = \"new york\"\n\nprint(\"Top 3 matches:\")\nfor choice, score, idx in process.extract(query, cities, limit=3):\n    print(f\"  {choice:20} score={score:5.1f}  (index {idx})\")\n\nprint()\nbest = process.extractOne(query, cities)\nprint(f\"Best match: {best}\")\n</pre> cities = [     \"New York\", \"New Orleans\", \"Newark\",     \"Los Angeles\", \"San Francisco\", \"Nashville\",     \"Boston\", \"Denver\", \"Miami\", ]  query = \"new york\"  print(\"Top 3 matches:\") for choice, score, idx in process.extract(query, cities, limit=3):     print(f\"  {choice:20} score={score:5.1f}  (index {idx})\")  print() best = process.extractOne(query, cities) print(f\"Best match: {best}\") In\u00a0[\u00a0]: Copied! <pre># Only keep matches with score &gt;= 80\nresults = process.extract(\"berlin\", cities, score_cutoff=80.0)\nprint(f\"Matches &gt;= 80 for 'berlin': {results}\")\n\n# Cutoff in raw scorer\ns = fuzz.ratio(\"hello\", \"world\", score_cutoff=90.0)\nprint(f\"ratio('hello','world', cutoff=90) = {s}  (below cutoff \u2192 0.0)\")\n</pre> # Only keep matches with score &gt;= 80 results = process.extract(\"berlin\", cities, score_cutoff=80.0) print(f\"Matches &gt;= 80 for 'berlin': {results}\")  # Cutoff in raw scorer s = fuzz.ratio(\"hello\", \"world\", score_cutoff=90.0) print(f\"ratio('hello','world', cutoff=90) = {s}  (below cutoff \u2192 0.0)\")"},{"location":"cookbook/01_introduction/#introduction-to-rustfuzz","title":"Introduction to rustfuzz\u00b6","text":"<p><code>rustfuzz</code> is a blazing-fast fuzzy string matching library for Python \u2014 implemented entirely in Rust via PyO3.</p> <p>This notebook covers the core building blocks:</p> <ul> <li><code>fuzz.ratio</code> \u2014 character-level similarity</li> <li><code>fuzz.partial_ratio</code> \u2014 substring matching</li> <li><code>fuzz.token_*</code> \u2014 word-order insensitive matching</li> <li><code>utils.default_process</code> \u2014 string normalisation</li> <li><code>process.extract</code> / <code>process.extractOne</code> \u2014 batch matching</li> </ul>"},{"location":"cookbook/01_introduction/#1-fuzzratio-character-level-similarity","title":"1. <code>fuzz.ratio</code> \u2014 Character-level similarity\u00b6","text":"<p>Returns a score from <code>0.0</code> to <code>100.0</code> based on the Indel (insert/delete) distance between two strings. Identical strings \u2192 100.0. Completely different \u2192 0.0.</p>"},{"location":"cookbook/01_introduction/#2-fuzzpartial_ratio-substring-matching","title":"2. <code>fuzz.partial_ratio</code> \u2014 Substring matching\u00b6","text":"<p>Finds the best matching window of the shorter string inside the longer one. Ideal when the needle is fully contained in the haystack.</p>"},{"location":"cookbook/01_introduction/#3-token-ratios-word-order-insensitive-matching","title":"3. Token ratios \u2014 Word-order insensitive matching\u00b6","text":"Function What it does <code>token_sort_ratio</code> Sorts tokens alphabetically, then computes <code>ratio</code> <code>token_set_ratio</code> Compares the intersection and remainder sets of tokens <code>token_ratio</code> <code>max(token_sort_ratio, token_set_ratio)</code> <p>Use these when word order shouldn't matter (e.g. address matching, name deduplication).</p>"},{"location":"cookbook/01_introduction/#4-utilsdefault_process-normalisation","title":"4. <code>utils.default_process</code> \u2014 Normalisation\u00b6","text":"<p>Lowercases the string and replaces non-alphanumeric characters with spaces. Pass it as the <code>processor</code> argument to any scorer for more robust matching.</p>"},{"location":"cookbook/01_introduction/#5-processextract-processextractone-batch-matching","title":"5. <code>process.extract</code> / <code>process.extractOne</code> \u2014 Batch matching\u00b6","text":"<p>Match a query against a list of choices and get the top results. Each result is a <code>(choice, score, index)</code> tuple.</p>"},{"location":"cookbook/01_introduction/#6-score-cutoffs","title":"6. Score cutoffs\u00b6","text":"<p>All scorers accept a <code>score_cutoff</code> parameter. Results below the threshold are returned as <code>0.0</code> (or excluded from <code>process.extract</code>).</p>"},{"location":"cookbook/02_advanced_matching/","title":"Advanced Matching with rustfuzz","text":"In\u00a0[\u00a0]: Copied! <pre>from rustfuzz.distance import (\n    Levenshtein, Hamming, Indel,\n    Jaro, JaroWinkler,\n    LCSseq, OSA, DamerauLevenshtein,\n    Prefix, Postfix,\n)\nfrom rustfuzz.distance._initialize import Editop, Editops, Opcode, Opcodes, MatchingBlock\nfrom rustfuzz import process\nimport rustfuzz.fuzz as fuzz\n\nprint(\"imports OK \u2705\")\n</pre> from rustfuzz.distance import (     Levenshtein, Hamming, Indel,     Jaro, JaroWinkler,     LCSseq, OSA, DamerauLevenshtein,     Prefix, Postfix, ) from rustfuzz.distance._initialize import Editop, Editops, Opcode, Opcodes, MatchingBlock from rustfuzz import process import rustfuzz.fuzz as fuzz  print(\"imports OK \u2705\") In\u00a0[\u00a0]: Copied! <pre>word_pairs = [(\"kitten\", \"sitting\"), (\"sunday\", \"saturday\"), (\"rust\", \"trust\")]\n\nmetrics = [Levenshtein, Hamming, Indel, OSA, DamerauLevenshtein]\nmetric_names = [\"Levenshtein\", \"Hamming\", \"Indel\", \"OSA\", \"DamerauLevenshtein\"]\n\nfor a, b in word_pairs:\n    print(f\"\\n\u2500\u2500 {a!r} vs {b!r} \u2500\u2500\")\n    for name, M in zip(metric_names, metrics):\n        try:\n            d = M.distance(a, b)\n            nd = M.normalized_distance(a, b)\n            print(f\"  {name:20}  distance={d:2d}  norm_dist={nd:.3f}\")\n        except Exception as e:\n            print(f\"  {name:20}  N/A ({e})\")\n</pre> word_pairs = [(\"kitten\", \"sitting\"), (\"sunday\", \"saturday\"), (\"rust\", \"trust\")]  metrics = [Levenshtein, Hamming, Indel, OSA, DamerauLevenshtein] metric_names = [\"Levenshtein\", \"Hamming\", \"Indel\", \"OSA\", \"DamerauLevenshtein\"]  for a, b in word_pairs:     print(f\"\\n\u2500\u2500 {a!r} vs {b!r} \u2500\u2500\")     for name, M in zip(metric_names, metrics):         try:             d = M.distance(a, b)             nd = M.normalized_distance(a, b)             print(f\"  {name:20}  distance={d:2d}  norm_dist={nd:.3f}\")         except Exception as e:             print(f\"  {name:20}  N/A ({e})\") In\u00a0[\u00a0]: Copied! <pre>name_pairs = [\n    (\"martha\", \"marhta\"),   # transposition\n    (\"dwayne\", \"duane\"),\n    (\"john\",   \"jon\"),\n    (\"smith\",  \"smythe\"),\n    (\"alice\",  \"bob\"),\n]\n\nprint(f\"{'A':10} {'B':10} {'Jaro':&gt;8} {'JaroWinkler':&gt;12}\")\nprint(\"-\" * 45)\nfor a, b in name_pairs:\n    j  = Jaro.similarity(a, b)\n    jw = JaroWinkler.similarity(a, b)\n    print(f\"{a:10} {b:10} {j:8.4f} {jw:12.4f}\")\n</pre> name_pairs = [     (\"martha\", \"marhta\"),   # transposition     (\"dwayne\", \"duane\"),     (\"john\",   \"jon\"),     (\"smith\",  \"smythe\"),     (\"alice\",  \"bob\"), ]  print(f\"{'A':10} {'B':10} {'Jaro':&gt;8} {'JaroWinkler':&gt;12}\") print(\"-\" * 45) for a, b in name_pairs:     j  = Jaro.similarity(a, b)     jw = JaroWinkler.similarity(a, b)     print(f\"{a:10} {b:10} {j:8.4f} {jw:12.4f}\") In\u00a0[\u00a0]: Copied! <pre>src = \"kitten\"\ndst = \"sitting\"\n\nops = Levenshtein.editops(src, dst)\nprint(f\"editops({src!r} \u2192 {dst!r}):\")\nfor op in ops:\n    print(f\"  {op}\")\n\nprint()\ncodes = Levenshtein.opcodes(src, dst)\nprint(f\"opcodes:\")\nfor block in codes:\n    print(f\"  {block}\")\n</pre> src = \"kitten\" dst = \"sitting\"  ops = Levenshtein.editops(src, dst) print(f\"editops({src!r} \u2192 {dst!r}):\") for op in ops:     print(f\"  {op}\")  print() codes = Levenshtein.opcodes(src, dst) print(f\"opcodes:\") for block in codes:     print(f\"  {block}\") In\u00a0[\u00a0]: Copied! <pre># Apply opcodes to visualise the diff\ndef visualise_diff(src: str, dst: str) -&gt; None:\n    codes = Levenshtein.opcodes(src, dst)\n    out = []\n    for block in codes:\n        tag = block.tag\n        s1, s2, d1, d2 = block.src_start, block.src_end, block.dest_start, block.dest_end\n        if tag == \"equal\":\n            out.append(src[s1:s2])\n        elif tag == \"replace\":\n            out.append(f\"[{src[s1:s2]}\u2192{dst[d1:d2]}]\")\n        elif tag == \"insert\":\n            out.append(f\"[+{dst[d1:d2]}]\")\n        elif tag == \"delete\":\n            out.append(f\"[-{src[s1:s2]}]\")\n    print(\"\".join(out))\n\nvisualise_diff(\"kitten\", \"sitting\")\nvisualise_diff(\"hello world\", \"hello cruel world\")\nvisualise_diff(\"New York City\", \"New Orleans\")\n</pre> # Apply opcodes to visualise the diff def visualise_diff(src: str, dst: str) -&gt; None:     codes = Levenshtein.opcodes(src, dst)     out = []     for block in codes:         tag = block.tag         s1, s2, d1, d2 = block.src_start, block.src_end, block.dest_start, block.dest_end         if tag == \"equal\":             out.append(src[s1:s2])         elif tag == \"replace\":             out.append(f\"[{src[s1:s2]}\u2192{dst[d1:d2]}]\")         elif tag == \"insert\":             out.append(f\"[+{dst[d1:d2]}]\")         elif tag == \"delete\":             out.append(f\"[-{src[s1:s2]}]\")     print(\"\".join(out))  visualise_diff(\"kitten\", \"sitting\") visualise_diff(\"hello world\", \"hello cruel world\") visualise_diff(\"New York City\", \"New Orleans\") In\u00a0[\u00a0]: Copied! <pre>same_len_pairs = [\n    (\"karolin\", \"kathrin\"),\n    (\"1011101\", \"1001001\"),\n    (\"GGACTGA\", \"GGCCTGA\"),\n]\n\nfor a, b in same_len_pairs:\n    d  = Hamming.distance(a, b)\n    ns = Hamming.normalized_similarity(a, b)\n    print(f\"{a!r} vs {b!r}  \u2192 distance={d}  norm_similarity={ns:.3f}\")\n</pre> same_len_pairs = [     (\"karolin\", \"kathrin\"),     (\"1011101\", \"1001001\"),     (\"GGACTGA\", \"GGCCTGA\"), ]  for a, b in same_len_pairs:     d  = Hamming.distance(a, b)     ns = Hamming.normalized_similarity(a, b)     print(f\"{a!r} vs {b!r}  \u2192 distance={d}  norm_similarity={ns:.3f}\") In\u00a0[\u00a0]: Copied! <pre>try:\n    import numpy as np\n    import pandas as pd\n\n    queries  = [\"New York\", \"Los Angeles\", \"Chicago\"]\n    choices  = [\"New York City\", \"Los Angeles\", \"Newark\", \"Chicago Heights\", \"New Orleans\"]\n\n    matrix = process.cdist(queries, choices, scorer=fuzz.ratio)\n\n    df = pd.DataFrame(matrix, index=queries, columns=choices)\n    print(df.to_string())\n\nexcept ImportError:\n    print(\"Install numpy + pandas: uv pip install rustfuzz[all] pandas\")\n</pre> try:     import numpy as np     import pandas as pd      queries  = [\"New York\", \"Los Angeles\", \"Chicago\"]     choices  = [\"New York City\", \"Los Angeles\", \"Newark\", \"Chicago Heights\", \"New Orleans\"]      matrix = process.cdist(queries, choices, scorer=fuzz.ratio)      df = pd.DataFrame(matrix, index=queries, columns=choices)     print(df.to_string())  except ImportError:     print(\"Install numpy + pandas: uv pip install rustfuzz[all] pandas\") In\u00a0[\u00a0]: Copied! <pre>import rustfuzz.utils as utils\n\nmessy = [\n    \"New York\", \"new york\", \"New-York\", \"N.Y.\",\n    \"Los Angeles\", \"los angeles\", \"L.A.\", \"Los Angles\",\n    \"Chicago\", \"chicago\", \"Chikago\",\n    \"Houston\", \"Huston\",\n]\n\nTHRESHOLD = 75.0\n\nclusters: list[list[str]] = []\nassigned: set[int] = set()\n\nfor i, name in enumerate(messy):\n    if i in assigned:\n        continue\n    cluster = [name]\n    assigned.add(i)\n    for j, other in enumerate(messy):\n        if j in assigned:\n            continue\n        score = fuzz.token_set_ratio(\n            utils.default_process(name),\n            utils.default_process(other),\n        )\n        if score &gt;= THRESHOLD:\n            cluster.append(other)\n            assigned.add(j)\n    clusters.append(cluster)\n\nfor c in clusters:\n    canonical = c[0]\n    dupes = c[1:]\n    print(f\"  {canonical!r:15} \u2190 {dupes}\")\n</pre> import rustfuzz.utils as utils  messy = [     \"New York\", \"new york\", \"New-York\", \"N.Y.\",     \"Los Angeles\", \"los angeles\", \"L.A.\", \"Los Angles\",     \"Chicago\", \"chicago\", \"Chikago\",     \"Houston\", \"Huston\", ]  THRESHOLD = 75.0  clusters: list[list[str]] = [] assigned: set[int] = set()  for i, name in enumerate(messy):     if i in assigned:         continue     cluster = [name]     assigned.add(i)     for j, other in enumerate(messy):         if j in assigned:             continue         score = fuzz.token_set_ratio(             utils.default_process(name),             utils.default_process(other),         )         if score &gt;= THRESHOLD:             cluster.append(other)             assigned.add(j)     clusters.append(cluster)  for c in clusters:     canonical = c[0]     dupes = c[1:]     print(f\"  {canonical!r:15} \u2190 {dupes}\")"},{"location":"cookbook/02_advanced_matching/#advanced-matching-with-rustfuzz","title":"Advanced Matching with rustfuzz\u00b6","text":"<p>This notebook covers the full distance API, edit operations, alignment objects, and <code>process.cdist</code> for pairwise matrices.</p> <p>Topics:</p> <ol> <li>The <code>rustfuzz.distance</code> API \u2014 <code>distance</code>, <code>similarity</code>, <code>normalized_*</code></li> <li>Edit-operation objects: <code>Editops</code>, <code>Opcodes</code>, <code>MatchingBlock</code></li> <li>Per-metric deep-dives: Levenshtein, Jaro-Winkler, DamerauLevenshtein, Hamming</li> <li><code>process.cdist</code> \u2014 pairwise distance matrices</li> <li>Real-world deduplication recipe</li> </ol>"},{"location":"cookbook/02_advanced_matching/#1-distance-api-overview","title":"1. Distance API Overview\u00b6","text":"<p>Every metric module exposes the same interface:</p> Method Returns Meaning <code>.distance(a, b)</code> <code>int</code> Raw edit distance (lower = closer) <code>.similarity(a, b)</code> <code>int</code> Matching characters / operations <code>.normalized_distance(a, b)</code> <code>float [0,1]</code> 0.0 = identical <code>.normalized_similarity(a, b)</code> <code>float [0,1]</code> 1.0 = identical"},{"location":"cookbook/02_advanced_matching/#2-jaro-jaro-winkler","title":"2. Jaro &amp; Jaro-Winkler\u00b6","text":"<p>Jaro-Winkler is particularly good for short strings and proper names \u2014 it gives extra weight to a common prefix.</p>"},{"location":"cookbook/02_advanced_matching/#3-edit-operations-editops-and-opcodes","title":"3. Edit Operations \u2014 <code>Editops</code> and <code>Opcodes</code>\u00b6","text":"<p><code>Levenshtein.editops(a, b)</code> returns the minimal list of operations (insert, delete, replace) to transform <code>a</code> into <code>b</code>.</p> <p><code>Levenshtein.opcodes(a, b)</code> returns contiguous blocks in a format compatible with <code>difflib</code>.</p>"},{"location":"cookbook/02_advanced_matching/#4-hamming-distance","title":"4. Hamming Distance\u00b6","text":"<p>Hamming distance counts positions where characters differ. It only works on strings of equal length.</p>"},{"location":"cookbook/02_advanced_matching/#5-processcdist-pairwise-distance-matrix","title":"5. <code>process.cdist</code> \u2014 Pairwise Distance Matrix\u00b6","text":"<p>Computes a full N\u00d7M matrix of similarity scores. Requires <code>numpy</code>.</p>"},{"location":"cookbook/02_advanced_matching/#6-real-world-recipe-deduplication","title":"6. Real-world recipe \u2014 Deduplication\u00b6","text":"<p>Group a list of messy city names into deduplicated clusters using <code>process.extract</code> and a score threshold.</p>"},{"location":"cookbook/03_benchmarks/","title":"Performance Benchmarks","text":"In\u00a0[\u00a0]: Copied! <pre>import timeit\nimport rustfuzz.fuzz as fuzz\nfrom rustfuzz.distance import Levenshtein, JaroWinkler, Hamming, DamerauLevenshtein\n\nN = 10_000\nS1, S2 = \"the quick brown fox jumps over the lazy dog\", \"the quick brown fox jumped over a lazy dog\"\n\nprint(f\"Strings:\\n  s1 = {S1!r}\\n  s2 = {S2!r}\\n\")\nprint(f\"Iterations: {N:,}\")\n</pre> import timeit import rustfuzz.fuzz as fuzz from rustfuzz.distance import Levenshtein, JaroWinkler, Hamming, DamerauLevenshtein  N = 10_000 S1, S2 = \"the quick brown fox jumps over the lazy dog\", \"the quick brown fox jumped over a lazy dog\"  print(f\"Strings:\\n  s1 = {S1!r}\\n  s2 = {S2!r}\\n\") print(f\"Iterations: {N:,}\") In\u00a0[\u00a0]: Copied! <pre>def bench(fn, *args, n=N) -&gt; float:\n    \"\"\"Return milliseconds per call (median of 5 runs).\"\"\"\n    times = timeit.repeat(lambda: fn(*args), number=n, repeat=5)\n    return min(times) / n * 1000  # ms per call\n\n\nbenchmarks = {\n    \"fuzz.ratio\":                lambda: fuzz.ratio(S1, S2),\n    \"fuzz.partial_ratio\":        lambda: fuzz.partial_ratio(S1, S2),\n    \"fuzz.token_sort_ratio\":     lambda: fuzz.token_sort_ratio(S1, S2),\n    \"fuzz.token_set_ratio\":      lambda: fuzz.token_set_ratio(S1, S2),\n    \"fuzz.WRatio\":               lambda: fuzz.WRatio(S1, S2),\n    \"Levenshtein.distance\":      lambda: Levenshtein.distance(S1, S2),\n    \"Levenshtein.normalized\":    lambda: Levenshtein.normalized_similarity(S1, S2),\n    \"JaroWinkler.similarity\":    lambda: JaroWinkler.similarity(S1, S2),\n    \"DamerauLevenshtein.dist\":   lambda: DamerauLevenshtein.distance(S1, S2),\n}\n\nresults: dict[str, float] = {}\n\nfor name, fn in benchmarks.items():\n    ms = bench(fn)\n    results[name] = ms\n    print(f\"  {name:35}  {ms*1000:.3f} \u03bcs/call\")\n\nprint(\"\\n\u2705 All benchmarks complete\")\n</pre> def bench(fn, *args, n=N) -&gt; float:     \"\"\"Return milliseconds per call (median of 5 runs).\"\"\"     times = timeit.repeat(lambda: fn(*args), number=n, repeat=5)     return min(times) / n * 1000  # ms per call   benchmarks = {     \"fuzz.ratio\":                lambda: fuzz.ratio(S1, S2),     \"fuzz.partial_ratio\":        lambda: fuzz.partial_ratio(S1, S2),     \"fuzz.token_sort_ratio\":     lambda: fuzz.token_sort_ratio(S1, S2),     \"fuzz.token_set_ratio\":      lambda: fuzz.token_set_ratio(S1, S2),     \"fuzz.WRatio\":               lambda: fuzz.WRatio(S1, S2),     \"Levenshtein.distance\":      lambda: Levenshtein.distance(S1, S2),     \"Levenshtein.normalized\":    lambda: Levenshtein.normalized_similarity(S1, S2),     \"JaroWinkler.similarity\":    lambda: JaroWinkler.similarity(S1, S2),     \"DamerauLevenshtein.dist\":   lambda: DamerauLevenshtein.distance(S1, S2), }  results: dict[str, float] = {}  for name, fn in benchmarks.items():     ms = bench(fn)     results[name] = ms     print(f\"  {name:35}  {ms*1000:.3f} \u03bcs/call\")  print(\"\\n\u2705 All benchmarks complete\") In\u00a0[\u00a0]: Copied! <pre>try:\n    import plotly.graph_objects as go\n\n    ops   = list(results.keys())\n    times = [v * 1000 for v in results.values()]  # \u03bcs\n\n    colors = [\n        f\"rgba({int(168 + i*4)},{int(85 - i*2)},{int(247 - i*10)},0.85)\"\n        for i in range(len(ops))\n    ]\n\n    fig = go.Figure(go.Bar(\n        x=ops,\n        y=times,\n        marker_color=colors,\n        text=[f\"{t:.2f} \u03bcs\" for t in times],\n        textposition=\"outside\",\n    ))\n    fig.update_layout(\n        title=\"rustfuzz \u2014 microseconds per call (lower is better)\",\n        xaxis_title=\"Operation\",\n        yaxis_title=\"\u03bcs / call\",\n        paper_bgcolor=\"#0f0319\",\n        plot_bgcolor=\"#1a0533\",\n        font=dict(color=\"#d8b4fe\"),\n        xaxis=dict(tickangle=-30),\n    )\n    fig.show()\n\nexcept ImportError:\n    print(\"Install plotly: uv pip install plotly\")\n    for name, ms in results.items():\n        bar = \"\u2588\" * int(ms * 1000 / max(results.values()) * 40)\n        print(f\"  {name:35} {bar} {ms*1000:.2f} \u03bcs\")\n</pre> try:     import plotly.graph_objects as go      ops   = list(results.keys())     times = [v * 1000 for v in results.values()]  # \u03bcs      colors = [         f\"rgba({int(168 + i*4)},{int(85 - i*2)},{int(247 - i*10)},0.85)\"         for i in range(len(ops))     ]      fig = go.Figure(go.Bar(         x=ops,         y=times,         marker_color=colors,         text=[f\"{t:.2f} \u03bcs\" for t in times],         textposition=\"outside\",     ))     fig.update_layout(         title=\"rustfuzz \u2014 microseconds per call (lower is better)\",         xaxis_title=\"Operation\",         yaxis_title=\"\u03bcs / call\",         paper_bgcolor=\"#0f0319\",         plot_bgcolor=\"#1a0533\",         font=dict(color=\"#d8b4fe\"),         xaxis=dict(tickangle=-30),     )     fig.show()  except ImportError:     print(\"Install plotly: uv pip install plotly\")     for name, ms in results.items():         bar = \"\u2588\" * int(ms * 1000 / max(results.values()) * 40)         print(f\"  {name:35} {bar} {ms*1000:.2f} \u03bcs\") In\u00a0[\u00a0]: Copied! <pre>import string, random\nrandom.seed(42)\n\ndef rand_str(n: int) -&gt; str:\n    return \"\".join(random.choices(string.ascii_lowercase, k=n))\n\nlengths = [10, 50, 100, 250, 500, 1000]\nscale_results: dict[int, float] = {}\n\nfor length in lengths:\n    a, b = rand_str(length), rand_str(length)\n    ms = bench(Levenshtein.distance, a, b, n=1000)\n    scale_results[length] = ms * 1000  # \u03bcs\n    print(f\"  len={length:5d}  {ms*1000:.3f} \u03bcs/call\")\n</pre> import string, random random.seed(42)  def rand_str(n: int) -&gt; str:     return \"\".join(random.choices(string.ascii_lowercase, k=n))  lengths = [10, 50, 100, 250, 500, 1000] scale_results: dict[int, float] = {}  for length in lengths:     a, b = rand_str(length), rand_str(length)     ms = bench(Levenshtein.distance, a, b, n=1000)     scale_results[length] = ms * 1000  # \u03bcs     print(f\"  len={length:5d}  {ms*1000:.3f} \u03bcs/call\") In\u00a0[\u00a0]: Copied! <pre>try:\n    import plotly.graph_objects as go\n\n    fig = go.Figure(go.Scatter(\n        x=list(scale_results.keys()),\n        y=list(scale_results.values()),\n        mode=\"lines+markers\",\n        line=dict(color=\"#a855f7\", width=3),\n        marker=dict(color=\"#22c55e\", size=10),\n    ))\n    fig.update_layout(\n        title=\"Levenshtein.distance \u2014 scaling by string length\",\n        xaxis_title=\"String length (chars)\",\n        yaxis_title=\"\u03bcs / call\",\n        paper_bgcolor=\"#0f0319\",\n        plot_bgcolor=\"#1a0533\",\n        font=dict(color=\"#d8b4fe\"),\n    )\n    fig.show()\n\nexcept ImportError:\n    for length, us in scale_results.items():\n        print(f\"  len={length:4d}  {us:.3f} \u03bcs\")\n</pre> try:     import plotly.graph_objects as go      fig = go.Figure(go.Scatter(         x=list(scale_results.keys()),         y=list(scale_results.values()),         mode=\"lines+markers\",         line=dict(color=\"#a855f7\", width=3),         marker=dict(color=\"#22c55e\", size=10),     ))     fig.update_layout(         title=\"Levenshtein.distance \u2014 scaling by string length\",         xaxis_title=\"String length (chars)\",         yaxis_title=\"\u03bcs / call\",         paper_bgcolor=\"#0f0319\",         plot_bgcolor=\"#1a0533\",         font=dict(color=\"#d8b4fe\"),     )     fig.show()  except ImportError:     for length, us in scale_results.items():         print(f\"  len={length:4d}  {us:.3f} \u03bcs\")"},{"location":"cookbook/03_benchmarks/#performance-benchmarks","title":"Performance Benchmarks\u00b6","text":"<p>This notebook measures <code>rustfuzz</code> performance using Python's <code>timeit</code> and visualises results with <code>plotly</code>.</p> <p>All benchmarks run N = 10,000 iterations on a fixed string pair to produce stable measurements.</p>"},{"location":"cookbook/03_benchmarks/#results-bar-chart","title":"Results \u2014 Bar Chart\u00b6","text":""},{"location":"cookbook/03_benchmarks/#scaling-benchmark-string-length","title":"Scaling benchmark \u2014 string length\u00b6","text":"<p>How does <code>Levenshtein.distance</code> scale with string length?</p>"},{"location":"cookbook/04_hybrid_search/","title":"Vector Databases &amp; Hybrid Search","text":"<p><code>rustfuzz</code> provides a lightning-fast native <code>BM25Okapi</code> search index that evaluates massive scale datasets dynamically outside the Python GIL.</p> <p>When combined with dense vector embeddings (e.g. OpenAI, Cohere) from a Vector Database, you can create a state-of-the-art Hybrid Search pipeline. Rustfuzz's <code>HybridSearch</code> class natively runs Reciprocal Rank Fusion (RRF) to fuse sparse keyword rankings and dense semantic similarities into a unified result limit.</p> <p>Below are comprehensive examples of integrating <code>rustfuzz</code> with your favorite Vector DB.</p>"},{"location":"cookbook/04_hybrid_search/#1-qdrant","title":"1. Qdrant","text":"<p>Qdrant is a high-performance, Rust-native vector database. Because both <code>rustfuzz</code> and <code>Qdrant</code> run on compiled native engines, they make an exceptionally fast hybrid pipeline.</p> <pre><code>from qdrant_client import QdrantClient\nfrom rustfuzz.search import HybridSearch\n\n# Initialize Qdrant and extract embeddings\nclient = QdrantClient(path=\"path/to/qdrant_db\")\nrecords = client.scroll(collection_name=\"my_documents\", limit=10000)[0]\n\ncorpus = [r.payload[\"text\"] for r in records]\nembeddings = [r.vector for r in records]\n\n# Initialize Rustfuzz Hybrid Search Engine\nsearch_engine = HybridSearch(corpus=corpus, embeddings=embeddings)\n\n# Query Qdrant for the vector \nquery_text = \"apple macbook pro\"\nquery_vector = embed_model.encode(query_text) # Using your embedding model\n\n# Run native RRF Fusion!\ntop_results = search_engine.search(query_text, query_embedding=query_vector, n=5)\n\nfor doc, score in top_results:\n    print(f\"[{score:.3f}] - {doc}\")\n</code></pre>"},{"location":"cookbook/04_hybrid_search/#2-lancedb","title":"2. LanceDB","text":"<p>LanceDB is an easy-to-use serverless vector database that natively integrates with Pandas and Polars.</p> <pre><code>import lancedb\nfrom rustfuzz.search import HybridSearch\n\ndb = lancedb.connect(\"~/.lancedb\")\ntable = db.open_table(\"documents\")\n\n# Read into Pandas\ndf = table.to_pandas()\ncorpus = df[\"text\"].tolist()\nembeddings = df[\"vector\"].tolist()\n\nsearch_engine = HybridSearch(corpus=corpus, embeddings=embeddings)\n\n# Run Query\nquery_vector = embed_model.encode(\"machine learning pipelines\")\nresults = search_engine.search(\n    query=\"machine learning pipelines\", \n    query_embedding=query_vector, \n    n=10, \n    rrf_k=60\n)\n</code></pre>"},{"location":"cookbook/04_hybrid_search/#3-faiss-meta","title":"3. FAISS (Meta)","text":"<p>FAISS is the classic library for efficient similarity search. Using FAISS for the dense retrieval and <code>rustfuzz</code> for sparse RRF provides extreme performance.</p> <pre><code>import faiss\nimport numpy as np\nfrom rustfuzz.search import HybridSearch\n\n# Assuming vectors is a numpy array of shape (N, D)\nvectors = np.random.random((10000, 384)).astype(\"float32\")\ncorpus = [f\"Document {i}\" for i in range(10000)]\n\n# Build FAISS Index\nindex = faiss.IndexFlatL2(384)\nindex.add(vectors)\n\n# Build Rustfuzz \nsearch_engine = HybridSearch(corpus=corpus, embeddings=vectors.tolist())\n\n# Query\nquery_vector = np.random.random((1, 384)).astype(\"float32\")\nresults = search_engine.search(\"document 100\", query_embedding=query_vector)\n</code></pre>"},{"location":"cookbook/04_hybrid_search/#4-pgvector-postgresql","title":"4. pgvector (PostgreSQL)","text":"<p>If your architecture is backed by Postgres, <code>pgvector</code> adds powerful vector search directly into SQL.</p> <pre><code>import psycopg2\nfrom rustfuzz.search import HybridSearch\n\nconn = psycopg2.connect(\"dbname=postgres user=postgres\")\ncur = conn.cursor()\n\n# Export from SQL\ncur.execute(\"SELECT text, embedding FROM documents\")\nrows = cur.fetchall()\n\ncorpus = [row[0] for row in rows]\nembeddings = [eval(row[1]) for row in rows] # Convert vector string array to list\n\nsearch_engine = HybridSearch(corpus=corpus, embeddings=embeddings)\n\n# Search\nresults = search_engine.search(\"database scaling\", query_embedding=query_vector)\n</code></pre>"},{"location":"cookbook/04_hybrid_search/#5-zvec-alibaba","title":"5. ZVec (Alibaba)","text":"<p>ZVec is an ultra-fast vector search engine. Integrate it seamlessly:</p> <pre><code>import zvec\nfrom rustfuzz.search import HybridSearch\n\n# Retrieve vectors from ZVec\ncollection = zvec.Collection(\"zvec_docs\")\ndata = collection.fetch_all()\n\nsearch_engine = HybridSearch(\n    corpus=[d.text for d in data], \n    embeddings=[d.vector for d in data]\n)\n\nresults = search_engine.search(\"alibaba cloud optimization\", query_embedding=query_vector)\n</code></pre>"},{"location":"cookbook/05_langchain/","title":"LangChain Integration","text":"<p><code>rustfuzz</code> provides a native <code>BaseRetriever</code> class that acts as a drop-in component for your LangChain <code>Runnable</code> pipelines.</p> <p>Because <code>rustfuzz</code> executes <code>BM25Okapi</code> indices entirely in compiled machine code, this LangChain retriever runs orders of magnitude faster than standard <code>langchain_community</code> Python implementations. </p>"},{"location":"cookbook/05_langchain/#installation","title":"Installation","text":"<p>Make sure you have both libraries available:</p> <pre><code>pip install rustfuzz langchain-core\n</code></pre>"},{"location":"cookbook/05_langchain/#basic-retrieval-pipeline","title":"Basic Retrieval Pipeline","text":"<p>You can instantiate <code>RustfuzzBM25Retriever</code> natively from strings or via <code>Document</code> objects.</p> <pre><code>from langchain_core.documents import Document\nfrom rustfuzz.langchain import RustfuzzBM25Retriever\n\n# 1. Prepare LangChain Documents\ndocs = [\n    Document(page_content=\"Apples and Oranges are fresh fruit.\", metadata={\"id\": 1}),\n    Document(page_content=\"The dog chased the cat down the street.\", metadata={\"id\": 2}),\n    Document(page_content=\"Apples are grown natively in Washington.\", metadata={\"id\": 3})\n]\n\n# 2. Build the Rustfuzz Retriever \n# (BM25 is evaluated instantaneously here in Rust memory space)\nretriever = RustfuzzBM25Retriever.from_documents(docs, k=2)\n\n# 3. Retrieve\nresults = retriever.invoke(\"Apples\")\n\nfor doc in results:\n    print(doc.page_content)\n    # The rustfuzz integration natively attaches the calculated BM25 float score \n    # to the `metadata[\"score\"]` property of every returned Document.\n    print(f\"BM25 Score: {doc.metadata['score']}\")\n</code></pre>"},{"location":"cookbook/05_langchain/#advanced-constructor-from-texts","title":"Advanced Constructor (From Texts)","text":"<p>If you are just streaming text and do not have initialized <code>Document</code> objects, you can skip the boilerplate using <code>.from_texts</code>:</p> <pre><code>texts = [\n    \"I love apples\",\n    \"Oranges are decent\"\n]\nmetadatas = [{\"source\": \"A\"}, {\"source\": \"B\"}]\n\nretriever = RustfuzzBM25Retriever.from_texts(\n    texts, \n    metadatas=metadatas, \n    k=1, \n    k1=1.5,   # BM25 tuning parameters natively passed to the core index\n    b=0.75\n)\n\nresults = retriever.invoke(\"apples\")\n</code></pre>"},{"location":"cookbook/05_langchain/#combining-with-rag-qa-chain","title":"Combining with RAG (Q&amp;A Chain)","text":"<p>Since <code>RustfuzzBM25Retriever</code> conforms fully to LangChain's standard Protocol, you can seamlessly wire it into Conversational Retrieval Chains.</p> <pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nllm = ChatOpenAI()\n\nprompt = ChatPromptTemplate.from_template(\"\"\"\nAnswer the question uniquely based on the provided context:\n{context}\n\nQuestion: {question}\n\"\"\")\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# The Rustfuzz Retriever acts as the data fetcher\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"Where are apples grown natively?\")\n</code></pre>"}]}