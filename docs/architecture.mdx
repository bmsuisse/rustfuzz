---
title: Architecture & Design
description: "How rustfuzz achieves maximum performance through Rust, PyO3, and smart algorithmic design."
---

`rustfuzz` is built with a singular goal: provide the fastest fuzzy string matching library for Python by moving all heavy computation into Rust, while maintaining a perfectly Pythonic API.

## 1. Rust FFI & PyO3 Bindings

The bridge between Python and Rust is built using `PyO3`. We prioritize zero-copy or minimal-copy extraction of Python strings.

### String Extraction

When a Python string is passed to a Rust function, we avoid allocating new Rust `String` objects if possible.

- For ASCII strings (`PyUnicode_1BYTE_KIND`), we extract direct pointers to the underlying byte array using `PyUnicode_AsUTF8AndSize` and process them as `&[u8]`.
- This ensures that operations like `fuzz.ratio` have **zero allocation overhead** for the strings themselves.

### Apache Arrow & PyCapsule Management

For high-throughput analytical workloads (e.g. PySpark DataFrames), zero-copy data transfer is facilitated using the Apache Arrow C-Data interface. We strictly manage `PyCapsule` lifetimes during FFI boundary crossings to ensure that memory references remain valid and are not prematurely garbage collected by Python, completely avoiding segmentation faults during batch processing.

## 2. Core Algorithms & Optimizations

The `src/algorithms.rs` and `src/fuzz.rs` files contain highly optimized versions of standard fuzzy matching algorithms.

<CardGroup cols={2}>
  <Card title="Levenshtein (Myers' bit-parallel)" icon="bolt">
    Uses `PatternMask64` for strings up to 64 characters — significantly outperforms naive DP approaches.
  </Card>
  <Card title="Pattern Masks" icon="microchip">
    64-bit integer masks per character allow multiple characters to be processed per CPU instruction.
  </Card>
  <Card title="Fast Paths" icon="route">
    Native scorers (`ratio`, `wratio`, `partial_ratio`) bypass Python callback overhead entirely in batch calls.
  </Card>
</CardGroup>

## 3. Batch Processing (`process.rs`)

Batch processing is the area where `rustfuzz` delivers the most significant performance gains over pure Python or Cython implementations.

### The Fast Path

When `process.extract` evaluates a query against a list of choices:

<Steps>
  <Step title="Scorer check">
    Validates if the selected scorer is implemented natively (`ScorerType`).
  </Step>
  <Step title="Native Fast Path">
    If the scorer is native and no custom Python `processor` is provided, it enters `ratio_fast`.
  </Step>
  <Step title="Parallel execution">
    For large lists (defined by `PARALLEL_THRESHOLD = 64`), switches to a Rayon-powered parallel model.
  </Step>
</Steps>

### Concurrency and the GIL

| Phase | GIL State | What happens |
|---|---|---|
| Phase 1 | **Held** | Safely extract raw byte slice pointers from the Python list |
| Phase 2 | **Released** | `py.allow_threads()` + Rayon `par_iter` — pure Rust math across all CPU cores |
| Phase 3 | **Held** | Pack winning `(PyObject, score, index)` tuples back into a Python list |

This architecture scales linearly with CPU cores for `cdist` and `extract_iter`.

## 4. Advanced Data Structures

### BK-Trees for Deduplication

Deduplication is handled natively in Rust using a Burkhard-Keller Tree (`src/distance/bktree.rs`). The tree organizes strings based on a discrete metric distance (Levenshtein). This turns an $O(N^2)$ deduplication problem into a vastly faster tree traversal where entire branches are pruned based on the triangle inequality.

### Hybrid Search & BM25

For integration with LLMs and Vector databases, `src/search.rs` provides a native `BM25Index`.

- Implements the Okapi BM25 scoring algorithm, parallelized using Rayon over document chunks.
- **RRF (Reciprocal Rank Fusion):** `get_top_n_rrf` merges traditional lexical token matching (BM25) with character-level fuzzy matching — providing robust retrieval even when queries include typos.
